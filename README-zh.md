# XIPUAI-to-openAI-compatible-API

## 目录
[English](README.md)/[简体中文](README-zh.md)
- [概述](#概述)
- [背景](#背景)
- [如何使用](#如何使用)
  - [依赖项概览](#依赖项概览)
  - [具体操作](#具体操作)
- [原理讲解](#原理讲解)
- [我们踩过的那些“天坑”与深刻教训](#我们踩过的那些天坑与深刻教训)
- [常见报错](#常见报错信息原因措施)
- [To do list](#to-do-list)

## 概述

把xipuAI网页服务转化成API调用服务，支持知识库，上下文，联网搜索，temperature···

点击视频链接预览效果 https://drive.google.com/file/d/1-OGXcUYPfYZpAO9FbemmOAfxXSMPdFfM/view?usp=sharing

## 背景
之前看到有人把 Google AI Studio 逆向成 openAI格式 的API服务，用以免费使用时兴的 2.5pro(https://github.com/CJackHwang/AIstudioProxyAPI)
我看到洗脚利物浦xipuAI平台上的 o3，sonnet3.7等昂贵模型，不由冒出了同样的想法。


## 如何使用

### 依赖项概览

- 需要xjtlu师/生账号
- 可[点此](libraries)查看库
- win11开发
- auth模块需使用chrome浏览器
- 使用对话客户端，比如[cherry studio](https://github.com/CherryHQ/cherry-studio.git)，或者[deepchat](https://github.com/ThinkInAIXYZ/deepchat.git)

### 具体操作

#### 环境搭建
- 把[环境配置](environment.yml)下载到本地的某个空文件夹
- 在该文件夹打开cmd
- 使用conda复制环境
  `conda env create -f environment.yml`
- 激活环境
  `conda activate genai_project`


#### 初始配置
- 在刚刚的窗口运行[密码配置模块](config.py)
  `python config.py`

  输入你的西浦内部账号密码。脚本会生成一个.env文件，储存你的信息。这样以后就不需要重复输入密码了， _保护好个人信息哦！！_
- 运行[获取令牌脚本](auth.py)
  `python auth.py`
  全程无需任何操作，等待终端提示即可。
  （电脑需要安装chrome浏览器）
- 启动[adapter服务](xjtlu_adapter_final.py)
  `uvicorn adapter:app --reload`

  同时会在项目文件夹内生成log文件夹，里面有日志
- 桌面客户端对接
  新建服务商，类型openAI compatible，apiKEY随便写几个英文字母，baseurl`http://127.0.0.1:8000/v1/chat/completions`。

  拉取提供的模型。模型列表是程序内置的，无法实时获取。直接填写模型id会被传递到web服务。

#### 尝试使用
新建一个会话，选择模型并尝试会话



## 原理讲解

### config.py

这只是一个简单的密码存储脚本。

用户用它输入一次用户名和密码，它写入根目录env文件。这样以后就可以自动读取，不用重复输入。

---
### auth.py

- 获取env文件里的用户名密码
- 启动临时chrome窗口
- 模拟用户填写密码，点击“log in”
- 截获所需的令牌，存储到env文件。

每次获取新令牌都会覆盖老令牌

---

### expire.py
使用base64解码同目录下`.env`的JWT_TOKEN，获得相关信息。仅用于探索，不影响实际使用效果。

---
### adapter.py


当桌面客户端（如cherry studio）向我们的适配器发起一次对话请求时，会发生以下一系列事件：

1.  **接收请求 (`chat_proxy`)**:
    *   FastAPI 接收到一个符合OpenAI格式的 `POST /v1/chat/completions` 请求。
    *   我们立即解析这个请求的JSON内容，里面包含了 `model`, `temperature`, `messages` 数组等所有信息。

2.  **创建独立会话 (`create_new_session`)**:
    *   这是整个流程的**基石**。我们**立即**向学校的 `saveSession` 接口发起一个`POST`请求。
    *   这个请求的Payload里，我们一次性把从客户端请求中获取的`model`, `temperature`等参数全部带上。
    *   **核心认知**: 我们最终发现，这个接口支持在创建会话的同时就配置好所有参数，一步到位。
    *   服务器返回一个全新的、独一无二的 `sessionId`。这个会话就像一个“一次性沙盒”，专门为本次对话服务。

3.  **构建完整上下文 (`prompt_parts` & `join`)**:
    *   我们读取`messages`数组里的所有内容（包括`system`, `user`, `assistant`的角色和对话）。
    *   我们采用最简单、最直接的方式，将它们拼接成一个巨大的、单一的文本字符串，格式为 `角色: 内容`，并用换行符隔开。
    *   **核心认知**: 我们最终确认，后端服务器非常强大，它能够自行处理这种包含大量原始文本、甚至含有Markdown和代码片段的“大杂烩”Prompt，我们无需在客户端做任何复杂的提示词工程或内容截断。

4.  **关键的策略性延迟 (`asyncio.sleep`)**:
    *   在成功创建会话并准备好`prompt`之后，我们**故意让程序暂停一小段时间**（`INTER_REQUEST_DELAY`，通常是1秒）。
    *   **核心认知**: 这是整个项目能够稳定运行的**命脉**。它解决了困扰我们许久的`429 Too Many Requests`问题。其根本原因是，服务器对**总的API请求频率**有限制，而不是针对单个接口。这个延迟将“创建会话”和“发送对话”这两个网络请求在时间上拉开了足够的距离，模拟了真实用户的操作间隔，从而完美地规避了速率限制。

5.  **发送对话并流式响应 (`stream_generator`)**:
    *   延迟结束后，我们向学校的 `completions` 接口发起一个**流式** `POST` 请求。
    *   Payload里包含了刚刚创建的 `sessionId` 和那个巨大的 `full_prompt`。
    *   我们实时地接收从服务器流过来的文本块，将它们重新包装成OpenAI格式的`chunk`，再实时地发回给桌面客户端。这保证了用户能看到“打字机”效果。

6.  **用后自动清理 (`finally` & `delete_session`)**:
    *   当对话流结束时（无论是正常完成还是中途出错），`stream_generator`中的 `finally` 块会被触发。
    *   **核心认知**: 我们发现服务器对用户能拥有的会话总数有限制（50个）。为了不耗尽这个资源池，我们必须“用完即焚”。
    *   `finally` 块会启动一个**后台异步任务**，调用 `delete_session` 函数。
    *   这个后台任务会向学校的 `delSession` 接口发起一个`POST`请求，Payload里包含了刚刚使用过的那个`sessionId`，将其从服务器上彻底删除。
    *   这个“垃圾回收”机制是“无状态”模式能长期运行的保证。

这个流程形成了一个完美的闭环：**创建 -> 使用 -> 销毁**，每一次对话都是一次全新的、独立的、不依赖服务器历史状态的交互，完全由桌面客户端掌控上下文。

---

### 我们踩过的那些“天坑”与深刻教训

这次旅程就像是在迷雾中航行，我们根据有限的线索制定航线，撞上冰山，再重新校准方向。

#### 坑1：上下文处理的“左右横跳”

*   **症状**: 模型似乎“失忆”，无法理解多轮对话，或者对包含知识库的长文本返回“我不懂”。
*   **最初的尝试 (v1-v4)**: 我们简单地将所有历史记录拼接成一个大字符串。这在简单对话中可行，但在处理复杂知识库时失败了。
*   **错误的转向 (v4 - “会话状态”模拟)**: 我们错误地认为应该模拟Web端的有状态行为，即每次只发送最后一条用户消息，并依赖服务器的`sessionId`来记住上下文。**这是我们最大的弯路**，因为它违背了我们构建“由客户端控制上下文”的无状态API的初衷。这导致模型完全没有上下文，自然无法正确回答。
*   **再次尝试 (v8 - “会话注入”)**: 我们想出了一个“聪明”的办法，试图通过多次POST请求来在服务器端一轮一轮地重建上下文。结果因为过于频繁的请求而失败。
*   **最终的顿悟 (v10)**: 你的观察——“Web端能一次性吞下大块文本”——让我们意识到，最简单的方法反而是正确的。问题不在于“如何发送上下文”，而在于**后端能处理什么样的上下文**以及**我们发送得有多快**。

#### 坑2：神秘的“异步”问题与429错误

*   **症状**: 频繁出现 `Request too fast` 或 `429 Too Many Requests` 错误。
*   **最初的尝试 (v2/v3 - “弹药库”模式)**: 我们设计了一个非常精巧的异步“弹药库”（`session_id_ammo_box`），在处理当前请求的同时，就去后台预取下一个会话ID。这个设计的本意是提高效率。
*   **致命的并发**: 问题在于，这个后台的“预取”请求（`saveSession`）和前台的“更新参数”请求（也是`saveSession`）在**毫秒级内同时发生**了。服务器的速率限制机制立刻将这种**并发访问**判定为异常，导致了失败。
*   **错误的解决方案 (v8 - “会话注入”)**: 我们废除了“弹药库”，但新的“会话注入”逻辑引入了更密集的请求序列，同样触发了速率限制。
*   **最终的顿悟 (v6, v10)**: 我们终于意识到，**问题的关键不是并发，而是频率**。我们不需要复杂的异步队列，只需要在**连续的API调用之间加入一个简单的、模拟人类行为的`asyncio.sleep()`**。这个小小的延迟，才是解决所有速率限制问题的“银弹”。

#### 坑3：对错误信息的误读

*   **症状**: 服务器返回的429错误信息里写着`...adjust the length of the context...`。
*   **错误的归因 (v9)**: 我们信了这条信息，认为是文本太长导致了问题。于是我们开发了“智能截断”模块。
*   **成功的巧合**: 这个截断模块虽然是基于错误的前提，但它**碰巧成功了**。我们后来才意识到，成功的原因不是截断本身，而是它附带的**请求延迟**，以及它可能**无意中清除了文本中的“毒性”字符**。
*   **最终的顿悟 (v10)**: 你的最终测试证明了，即使不截断，只要有延迟，后端也能处理长文本。这告诉我们一个深刻的教训：**API返回的错误信息不一定能准确地揭示问题的根本原因**，它有时会是“烟雾弹”。逆向工程需要大胆假设，更要小心求证。

## 常见报错信息，原因，措施
|报错|原因|措施|
|--|--|--|
|`403`|令牌过期|重新运行auth.py|
|`don't have relevant knowledge`|输入“毒文本”，后端无法阅读|删除该会话最后一次对话|

## To do list
- [ ] 自动化保活
